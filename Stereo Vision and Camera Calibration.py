# -*- coding: utf-8 -*-
"""vpaladug_project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AuCYd_VYZaLuma4JNlvVVCw3_pUoNWwn

Import all the libraries needed for this project.
"""

import cv2
import numpy as np
from google.colab import drive
import glob
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
drive.mount('/content/drive', force_remount=True)

"""I thought it would be easier to record a video and exract the frames from it. But there might some blurry frames so I am using laplacian to remove the blurry frames. The code below does that. It also prints how many frames are extracted which can be useful in determining the threshold value."""

def variance_of_laplacian(image):
    return cv2.Laplacian(image, cv2.CV_64F).var()

def extract_clear_frames(video_path, threshold=97.0, save_path='frames'):
    # Open the video
    cap = cv2.VideoCapture(video_path)
    count = 0

    while True:
        # Read a new frame
        ret, frame = cap.read()
        if not ret:
            break  # No more frames or error

        # Convert to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Calculate the sharpness
        fm = variance_of_laplacian(gray)

        # Check if the frame is clear enough
        if fm > threshold:
            # Save the frame
            cv2.imwrite(f"/content/drive/MyDrive/ENPM673/proj3/frame_{count}.png", frame)
            count += 1

    cap.release()
    print(f"Extracted {count} clear frames.")

# Example usage
extract_clear_frames('/content/drive/MyDrive/ENPM673/proj3/checker.mp4')

"""Google drive link for images: [https://drive.google.com/drive/folders/1VgyZ4GoOgYjdzgO3DKA04L58UUKqopil?usp=drive_link](https://)

I chose a chessboard for this project which has 9x6 internally corners.

**Pipeline:**
*  The video is first loaded from the google drive it is then processed for frames and filtered out using laplacian variance. The images are saved in a google drive folder and for all the next steps only this folder is used.
*   First, we store the cordinates of the chessboard corners in a 3D space, as it lies on a straight plane the z cordinate is always zero. It is stored in objp which acts as an example for determining the intrinsic and extrinsic properties of the camera. An objp-fitting coordinate grid is created by np.indices() and subsequently reshaped.

*   I setup 2 falgs to print out an example for undistorted image and original image which are used later.

*   Next images are read and converted to grayscale using cv2.imread and cv2.cvtcolor respectively. The code following this is just to print a original image.
*   Now corners of the chessboard are detected by cv2.findchessboardcorners which is an inbuilt functtion which I thought is very much made for this task. If corner is detected then objp is stored in a list object_points and corners in image_points list.


*   The next step is to callibrate camera, this is performed by using the funcion cv2.cameracalibrate this calculates the camera parameters and distortion.


*   Now we have a calibrated but distored image which has to be undistorted to acheive this we use cv2.undistort but before this we need a new camera matrix after calibration for this use cv2.getoptimalnewcameramatrix which is an inbuilt function. Then you undistort it using the opencv function.
*   Then we viuslise the corners found on undistorted and distorted images. I am printing the corners detected in the undistorted images in blue and corners on original image in yellow onto the original image.


**Functions Used:**


*   cv2.calibrateCamera: It is essential for obtaining exact camera settings required in many computer vision applications. Using known 3D real-world locations and the related 2D image projections, it calculates the intrinsic matrix and distortion coefficients of the camera. The usual placement of these dots is on a chessboard-like calibration pattern. Making it suitable for our task. The output consists of the distortion coefficients, which account for lens distortion, and the camera matrix, which comprises focal lengths and optical centre coordinates.
*   cv2.findChessboardcorners: The findChessboardCorners module is essential for camera calibration and 3D reconstruction because it offers a fast and precise way to identify the corners of a checkerboard pattern that is being used as a calibration object. This makes the function perfecct for our use case. It also has inbuilt feature of adjusting different lighting settings in the images which gives better outputs. The picture with the chessboard pattern in it is the main input for cv2.findChessboardCorners. For the purpose of making the detecting procedure easier, this picture is often transformed to grayscale. The number of inner corners per chessboard row and column—also known as the pattern size—must also be specified by the user. For example, an 8x6 pattern size would equate to eight corners per row and six per column as the corners on the board's edge are not included in this. The detection procedure may be improved by users by utilising a number of extra flags. A boolean value indicating whether corner detection was successful or unsuccessful and an array containing the coordinates of the corners that were identified are the two main outputs of the function.


*   cv2.getOptimalNewCameraMatrix: Reducing lens distortion and increasing the useful area of the image is one of the main benefits of utilising cv2.getOptimalNewCameraMatrix. Increasing the quality of images is essential for uses where accuracy and clarity are critical, such 3D reconstruction, augmented reality, and precise measurements. The size of the picture taken from the camera, the distortion coefficients and initial camera matrix from cv2.calibrateCamera, and an alpha parameter that regulates the free scaling factor are the main inputs that the method uses to work. As the name suggests it gives out a new camera matrix. Recalculating the camera matrix to best meet the input parameters' requirements for undistortion is what happens. The amount of distortion that was visible in the initial calibration, the size of the photographs, and the chosen alpha value—which determines how much the image field of vision is traded off with the existence of black pixels from undistortion—are all taken into account.
*   cv2.undistort: It is used for undistorting the the distorted image. It takes source image, original camera matrix, distortion coefficient and new camera matrix. It give out an undistorted image. It remaps the pixels of the incoming picture using the camera matrix and distortion coefficients. The optimal pixel coordinates are calculated using the intrinsic and distortion properties of the camera, and the undistorted picture is then created from the original image by straightforward interpolation. By remapping the image, the lines that would normally be twisted and curled due to lens distortions are made straight, the way they would appear in reality. This might include performing intricate mathematical procedures to solve such lens distortion model equations and ascertain where each pixel in the undistorted picture corresponds to in the distorted image.
"""

# Define the size of the chessboard
board_size = (9, 6)

# Create an array to store the 3D object points
objp = np.zeros((np.prod(board_size), 3), np.float32)
objp[:,:2] = np.indices(board_size).T.reshape(-1, 2)

# Arrays to store object points and image points from all images
objpts = []  # 3D points in real-world space
imgpts = []  # 2D points in image plane

# Load images
images = glob.glob('/content/drive/MyDrive/ENPM673/proj3/images/*.png')

if not images:
    raise ValueError("No images found. Check your file path.")

first_undistorted_displayed = False
first_original_displayed = False

# Loop through each image
for fname in images:
    img = cv2.imread(fname)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    if not first_original_displayed:
        # Display the first original image before undistortion
        print("THE FIRST ORIGINAL IMAGE BEFORE UNDISTORTION:")
        cv2_imshow(img)
        cv2.waitKey(5)  # Wait 500 ms
        cv2.destroyAllWindows()  # Close the window after displaying
        first_original_displayed = True

    # Find the chessboard corners
    ret, corners = cv2.findChessboardCorners(gray, board_size, None)

    # If corners are found, add object points and image points
    if ret:
        objpts.append(objp)
        imgpts.append(corners)

# Proceed with calibration only if we have collected data
if objpts and imgpts:
    # Calibrate the camera
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpts, imgpts, gray.shape[::-1], None, None)

    # Undistortion and corner detection visualization
    for fname in images:
        img = cv2.imread(fname)
        h, w = img.shape[:2]

        # Get the optimal new camera matrix and region of interest
        new_mat, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))

        # Undistort the image
        dst = cv2.undistort(img, mtx, dist, None, new_mat)

        if not first_undistorted_displayed:
            # Display the first undistorted image
            print("THE FIRST IMAGE AFTER UNDISTORTION:")
            cv2_imshow(dst)
            cv2.waitKey(5)  # Wait 500 ms
            cv2.destroyAllWindows()  # Close the window after displaying
            first_undistorted_displayed = True
            print("THE CORNERS DETECTED IN UNDISTORTED AND ORIGINAL IMAGE ON ORIGINAL IMAGE:")

        # Draw corners on the undistorted image
        ret_ud, corners_ud = cv2.findChessboardCorners(dst, board_size, None)

        # Draw corners on the original image for comparison
        ret, corners_orig = cv2.findChessboardCorners(img, board_size, None)

        if ret and ret_ud:
            dist_x_orig_img = img.copy()

            # Draw corners on the original image in yellow
            for corner in corners_orig:
                cv2.circle(dist_x_orig_img, (int(corner[0][0]), int(corner[0][1])), 5, (0, 255, 255), -1)

            # Draw undistorted corners in green
            for corner in corners_ud:
                cv2.circle(dist_x_orig_img, (int(corner[0][0]), int(corner[0][1])), 5, (255, 255, 0), -1)

            cv2_imshow(dist_x_orig_img)
            cv2.waitKey(500)
            cv2.destroyAllWindows()

        print("This is the new camera matrix:", new_mat)
        print("This is the old camera matrix:", mtx)
else:
    print("Could not find enough chessboard corners in any image for calibration.")

"""Now I calculated the reprojection error. This block of code uses most of the values from the above bloc.
**Pipeline:**



*   First an empty list is created, the average reprojection error for every picture used to calibrate the camera will be kept on file in this list. This first stage involves creating the data structure that will store the error values that are calculated for every group of points throughout the course of several calibration pictures.
*  Now we loop over the callibration data. This loop goes over each set of calibration data points is the central component of the script. Typically, these sets are made up of 3D world coordinates, or objpts, that match particular features—like chessboard corners—in the calibration pictures. Utilising the intrinsic camera matrix (mtx), distortion coefficients (dist), and predicted rotation and translation vectors (rvecs) of the camera, the script projects the 3D points back into the 2D picture plane for each iteration.

*   The cv2.projectPoints function is run during each loop iteration in order to convert the 3D object points into 2D picture points depending on the calibration pattern's posture and the supplied camera configurations. imgpts2, or the reprojected 2D points, are the result of this function. The script then calculates the Euclidean distance (L2 norm) between these reprojected locations and the 2D points that were first seen (imgpts). By normalising the error for each picture and dividing by the total number of points, the average reprojection error in pixels for that specific image is obtained. The previous average offers a numerical assessment of the degree to which the camera parameters facilitate the exact re-projection of 3D points to their respective positions in 2D images.
*  We then visulise the projection error using a scatter plot is used to create this visualisation, with each point denoting the reprojection error for a single calibration picture. With axes denoting the picture number and the error amount in pixels, the graphic is suitably labelled. With the ability to visually summarise the calibration quality throughout the whole collection of photos and identify any patterns or outliers in the reprojection error, this graphical depiction is quite helpful.


*   I used cv2.norm for error assement as it +is a flexible tool that measures the length or magnitude of the vector that the array represents. It may be used to compute the norm of an array or the difference between two arrays. Applications like feature normalisation, error assessment, and picture comparison can all benefit from this function.

**Significance of reprojection error:**

One of the most important metrics in camera calibration is the projection error, which is the difference between the projected and observed image points that are projected back onto the picture plane of the camera using the estimated camera parameters, after being projected from 3D space. As a quantitative indication of the precision of the camera calibration, this error is commonly expressed in pixels. If you want to assess the performance of any vision system that depends on precise camera calibration, you must comprehend the importance and consequences of reprojection inaccuracy.

Reprojection error is a crucial measure for measuring a camera's calibration accuracy. The difference represents the degree of precision with which the extrinsic parameters—like as the camera's orientation and location in relation to a known object—and intrinsic parameters—like focal length and optical center—have been determined. Rejection error is a measure of how well the calibration procedure captured the key features of the camera. A smaller reprojection error means that the camera's real qualities are more accurately represented.

Reprojection error minimization is a key objective in the camera calibration process. The reprojection error is minimised to guarantee that the camera settings appropriately match the observed data using repeated optimisation procedures.

**Implications:**

For activities that come after in computer vision applications, the precision of the camera calibration is consequently particularly important. Accurate measurement data extraction from the photos is necessary for several applications, such as 3D reconstruction, augmented reality, robot navigation, and more. The list is endless. The correct operation and dependability of the application will be jeopardised by calibration faults that spread to produce inaccuracies.

In order to match and identify characteristics across several pictures, calibration is a fine-tuning process characterised by minimal reprojection errors. The comprehension of 3D structure from many pictures is crucial for applications like stereo vision, which deduces motion from structures. Accurate calibration maintains consistency by guaranteeing that characteristics identified in different photos match.
"""

reprojection_errors = []

for i in range(len(objpts)):
    imgpts2, _ = cv2.projectPoints(objpts[i], rvecs[i], tvecs[i], mtx, dist)
    error = cv2.norm(imgpts[i], imgpts2, cv2.NORM_L2) / len(imgpts2)
    reprojection_errors.append(error)

# Plotting the reprojection errors
plt.figure()
plt.plot(reprojection_errors, marker='o')
plt.title('Reprojection Errors')
plt.xlabel('Image Number')
plt.ylabel('Error (pixels)')
plt.grid(True)
plt.show()

"""Now I am comparing the projected points with the original corners detected using this block of code
**Pipeline:**


*   First I iterated through hte object points, the quantity of objpts, or the number of pictures in which characteristics (such as chessboard corners) are effectively recognised, dictates the number of iterations. Then we load the images for processing.
*   Now the projection points are calculated using cv2.projecPoints, Utilising the rotation vectors (rvecs[i]), translation vectors (tvecs[i]), camera matrix (mtx), and distortion coefficients (dist), cv2.projectPoints is used to project the 3D object points (objpts[i]) onto the 2D picture plane. The function generates imgpts2, which represent the reprojected points' 2D coordinates, according to the calculated camera settings.




"""

for i in range(len(objpts)):  # Loop through the number of successful detections
    img = cv2.imread(images[i])  # Ensure this index is also safe to use

    # Project points
    imgpts2, _ = cv2.projectPoints(objpts[i], rvecs[i], tvecs[i], mtx, dist)

    # Draw original corners in red
    for corner in imgpts[i]:
        cv2.circle(img, (int(corner[0][0]), int(corner[0][1])), 5, (255, 0, 255), -1)

    # Draw reprojected points in green
    for point in imgpts2:
        cv2.circle(img, (int(point[0][0]), int(point[0][1])), 5, (0, 255, 255), -1)

    cv2_imshow(img)
    cv2.waitKey(500)

cv2.destroyAllWindows()

"""What's happening in my code:


*   First I wrote a function to collect the data from the calib.txt file. It creates a dictionary of the data provided.
*   Now you need to load images from the folders


*   Now I perform feature matching using SIFT and FLANN matcher, because bf matcher might be exhaustive and slow.

*   Fundamental matrix is calculated which determines the epipolar geometry between the images. I used RANSAC for this.

*  Now essetial matrix is calculate and also decomposed to get the rotation and translation matrices it uses fundamental matrix and calib data for this
*   Now the main part(rectification) is performed where by using a rectification transformation, the function aligns the uncalibrated stereo pictures so that similar points show up on the same rows in both images. Using the basic matrix, homography matrices for both images are computed. The rectified images are given out and used for depth and disparity map


*   The rectified images are used to compute disparity mapp first which is then used to create a depth map.


*   At the end epipolar lines are drawn and visualisation of depth and disparity map are performed.


*   The code takes user input for the given test cases. The inputs should be classroom, storageroom or traproom.
*   Also the images are taken in grayscale for better processing.




"""

def read_calib_txt_data(file_path):
    """
    Read calibration data from a file and store it in a dictionary.

    Parameters:
    file_path (str): The path to the file containing the calibration data.

    Returns:
    dict: A dictionary containing the calibration parameters.

    """
    calibration_params = {}
    with open(file_path, 'r') as file:
        for line in file:
            key, value = line.split('=')
            if 'cam' in key:
                matrix_values = value.strip().replace('[', '').replace(']', '').replace(';', ' ')
                calibration_params[key.strip()] = np.fromstring(matrix_values, sep=' ').reshape(3, 3)
            else:
                calibration_params[key.strip()] = float(value.strip())

    return calibration_params

def load_imgs(base_path):
    """
    Load left and right stereo images from the specified base path.

    Parameters:
    - base_path: The base path where the stereo images are located.

    Returns:
    - left_image: The left stereo image loaded as grayscale.
    - right_image: The right stereo image loaded as grayscale.
    """
    left_image_path = f"{base_path}/im0.png"
    right_image_path = f"{base_path}/im1.png"
    left_image = cv2.imread(left_image_path, cv2.IMREAD_GRAYSCALE)
    right_image = cv2.imread(right_image_path, cv2.IMREAD_GRAYSCALE)

    return left_image, right_image

def feature_matching(left_image, right_image):
    """
    Perform feature matching between left and right stereo images using SIFT.

    Parameters:
    - left_image: The left stereo image.
    - right_image: The right stereo image.

    Returns:
    - good_matches: List of good matches.
    - src_pts: Source points.
    - dst_pts: Destination points.
    """
    sift = cv2.SIFT_create()
    keypts1, descriptors1 = sift.detectAndCompute(left_image, None)
    keypts2, descriptors2 = sift.detectAndCompute(right_image, None)
    flann = cv2.FlannBasedMatcher()
    matches = flann.knnMatch(descriptors1, descriptors2, k=2)
    good_matches = []
    for m, n in matches:
        if m.distance < 0.75 * n.distance:
            good_matches.append(m)
    src_pts = np.float32([keypts1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([keypts2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

    return good_matches, src_pts, dst_pts

def estimate_fun_mat(matches, src_points, dst_points):
    """
    Estimate the fundamental matrix using the RANSAC algorithm.

    Parameters:
    - matches: List of good matches.
    - src_points: Source points.
    - dst_points: Destination points.

    Returns:
    - F: The estimated fundamental matrix.
    """
    F, mask = cv2.findFundamentalMat(src_points, dst_points, cv2.FM_RANSAC, 2, 0.99)
    return F

def compute_essential_and_decompose(F, calibration_params):
    """
    Compute the essential matrix and decompose it into rotation matrices and translation vector.

    Parameters:
    - F: The fundamental matrix.
    - calibration_params: Calibration parameters.

    Returns:
    - E: The essential matrix.
    - R1: First rotation matrix.
    - R2: Second rotation matrix.
    - t: Translation vector.
    """
    K = np.array(calibration_params['cam0']).reshape(3, 3)
    E = K.T @ F @ K
    decomposed = cv2.decomposeEssentialMat(E)
    if len(decomposed) == 3:
        R1, R2, t = decomposed
    else:
        _, R1, R2, t = decomposed
    return E, R1, R2, t

def rectify_uncalibrated(left_image, right_image, matches, F, calib_data, src_points, dst_points):
    """
    Rectify uncalibrated images using homography.

    Parameters:
    - left_image: The left stereo image.
    - right_image: The right stereo image.
    - matches: List of good matches.
    - F: The fundamental matrix.
    - calib_data: Calibration data.
    - src_points: Source points.
    - dst_points: Destination points.

    Returns:
    - rectified_left: The rectified left image.
    - rectified_right: The rectified right image.
    - H1: Homography matrix for left image.
    - H2: Homography matrix for right image.
    """
    _, H1, H2 = cv2.stereoRectifyUncalibrated(src_points, dst_points, F, imgSize=(left_image.shape[1], left_image.shape[0]))
    rectified_left = cv2.warpPerspective(left_image, H1, (left_image.shape[1], left_image.shape[0]))
    rectified_right = cv2.warpPerspective(right_image, H2, (right_image.shape[1], right_image.shape[0]))
    return rectified_left, rectified_right, H1, H2

def compute_disparity_and_depth(left_rectified, right_rectified, calibration_params):
    """
    Compute the disparity map and depth map using the StereoSGBM algorithm.

    Parameters:
    - left_rectified: The rectified left image.
    - right_rectified: The rectified right image.
    - calibration_params: Calibration parameters.

    Returns:
    - disparity_map: The disparity map.
    - depth_map: The depth map.
    """
    min_disp = int(calibration_params['vmin'])  # Minimum possible disparity
    ndisp = int(calibration_params['ndisp'])    # Number of possible disparities
    stereo = cv2.StereoSGBM_create(
        minDisparity=min_disp,
        numDisparities=ndisp,
        blockSize=5,
        P1=8 * 3 * 5 ** 2,
        P2=32 * 3 * 5 ** 2,
        disp12MaxDiff=1,
        uniquenessRatio=15,
        speckleWindowSize=0,
        speckleRange=2,
        preFilterCap=63,
        mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
    )
    disparity_map = stereo.compute(left_rectified, right_rectified)
    focal_length = calibration_params['cam0'][0, 0]
    baseline = calibration_params['baseline']
    with np.errstate(divide='ignore', invalid='ignore'):
        depth_map = (focal_length * baseline) / disparity_map
        depth_map[disparity_map == 0] = 0
    return disparity_map, depth_map


def draw_epipolar_lines(img1, img2, pts1, pts2, F):
    """
    Draw epipolar lines on both images given image points and a fundamental matrix.

    Parameters:
    - img1: The first image.
    - img2: The second image.
    - pts1: Image points from the first image.
    - pts2: Image points from the second image.
    - F: The fundamental matrix.

    Returns:
    - img1: The first image with epipolar lines.
    - img2: The second image with epipolar lines.
    """
    lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2, F)
    lines1 = lines1.reshape(-1, 3)
    img1_epilines = img1.copy()
    for r in lines1:
        color = tuple(np.random.randint(0, 255, 3).tolist())
        x0, y0 = map(int, [0, -r[2]/r[1]])
        x1, y1 = map(int, [img1.shape[1], -(r[2]+r[0]*img1.shape[1])/r[1]])
        img1 = cv2.line(img1, (x0, y0), (x1, y1), color, 1)
    lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)
    lines2 = lines2.reshape(-1, 3)
    img2_epilines = img2.copy()
    for r in lines2:
        color = tuple(np.random.randint(0, 255, 3).tolist())
        x0, y0 = map(int, [0, -r[2]/r[1]])
        x1, y1 = map(int, [img2.shape[1], -(r[2]+r[0]*img2.shape[1])/r[1]])
        img2 = cv2.line(img2, (x0, y0), (x1, y1), color, 1)
    return img1, img2

def display_visualization(image, window_name, colormap=cv2.COLORMAP_JET):
    """
    Display visualization of an image.

    Parameters:
    - image: The image to be displayed.
    - window_name: The name of the window.
    - colormap: The colormap to be applied.

    Returns:
    None
    """
    normalized_img = cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    normalized_img = np.uint8(normalized_img)
    colored_img = cv2.applyColorMap(normalized_img, colormap)
    cv2_imshow(normalized_img)
    cv2_imshow( colored_img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# This script assumes that required imports, such as cv2 and numpy, are already included in your code.
import sys

# Helper function to process a given dataset
def process_dataset(base_path, calib_path):
    # Read calibration data
    calib_data = read_calib_txt_data(calib_path)

    # Load stereo images
    left_img, right_img = load_imgs(base_path)

    # Perform feature matching
    matches, src_points, dst_points = feature_matching(left_img, right_img)

    # Estimate fundamental matrix
    F = estimate_fun_mat(matches, src_points, dst_points)

    # Compute essential matrix and decompose
    E, R1, R2, t = compute_essential_and_decompose(F, calib_data)

    # Rectify uncalibrated images
    rectified_left, rectified_right, H1, H2 = rectify_uncalibrated(
        left_img, right_img, matches, F, calib_data, src_points, dst_points
    )

    # Compute disparity map and depth map
    disparity, depth = compute_disparity_and_depth(rectified_left, right_img, calib_data)

    # Draw epipolar lines
    epipolar_1, epipolar_2 = draw_epipolar_lines(rectified_left, rectified_right, src_points, dst_points, F)

    # Display outputs
    print("Epipolar image for left rectified:")
    cv2_imshow(epipolar_1)

    print("Epipolar image for right rectified:")
    cv2_imshow(epipolar_2)

    print("Disparity map in grayscale and color:")
    display_visualization(disparity, "disparity")

    print("Depth map in grayscale and color:")
    display_visualization(depth, "depth")

    # Print homography matrices
    print("H1 matrix:", H1)
    print("H2 matrix:", H2)

# Main function to process a specific dataset
def main(dataset):
    # Process the specified dataset
    if dataset == "classroom":
        process_dataset(
            "/content/drive/MyDrive/ENPM673/proj3/problem-2/classroom",
            "/content/drive/MyDrive/ENPM673/proj3/problem-2/classroom/calib.txt",
        )
    elif dataset == "storageroom":
        process_dataset(
            "/content/drive/MyDrive/ENPM673/proj3/problem-2/storageroom",
            "/content/drive/MyDrive/ENPM673/proj3/problem-2/storageroom/calib.txt",
        )
    elif dataset == "traproom":
        process_dataset(
            "/content/drive/MyDrive/ENPM673/proj3/problem-2/traproom",
            "/content/drive/MyDrive/ENPM673/proj3/problem-2/traproom/calib.txt",
        )
    else:
        print("Unknown dataset:", dataset)

# Run the script
if __name__ == "__main__":
    # Get the dataset name from command-line arguments

    dataset = input("Enter the dataset name (classroom, storageroom, traproom): ")

    main(dataset)